Intro
========================================================
transition: fade
author: Lars Vilhuber
date: `r format(Sys.Date(), '%Y-%m-%d')`
autosize: true
width: 1440

Computing in social science research
========================================================

```{r setup,include=FALSE}
# this works best if this chunk is run manually first, but should work generally as well.
source(file.path(rprojroot::find_root(rprojroot::has_file("pathconfig.R")),"pathconfig.R"),echo=TRUE)
source(file.path(programs,"config.R"), echo=FALSE)
local.libraries <- c("latex2exp","venneuler")

results <- sapply(as.list(local.libraries), pkgTest)

```

High performance
----
may be optional

***

Reproducible
----
should never be

Defining reproducibility
========================
= replicable/ robust/ transparent/ generalizable

![Replicability](replicability.png)

Defining reproducibility
========================
= replicable/ robust/ transparent/ generalizable

- A formal definition 
- For now: __Robustness__
  - can run anywhere
  - can run without manual intervention
  - can run without deep knowledge of what is happening
  - has everything that is needed to run

Defining high-performance computing
===================================
- Requires more than your laptop
  - memory (4GB -> 4TB?)
  - CPUs (4 cores -> 1000s? millions?)
  - storage (200kB -> 200TB?)
  
Solutions for high-performance computing
========================================
- have a LAN Party
- buy a bigger laptop
- buy a bigger computer
- build your own computer
- University services
- National Research services
- Global (commercial) services (not all are legal...)

Connection between reproducibility and HPC
=========================================
- should be clear now
- your LAN party is not interested in learning your economic model - but they have the computers!
  - can run on everybody's computer
  - does not require manual intervention 
  - does not require knowledge of what's happening

Moving to third-party services 
==============
- you may need to move the **code** and/or **data**
- you may need to identify the exact **code** and/or **data** needed
- how long does it run?
- how does it scale? 

Scalability
========================================================
Processing times for perfectly (or "embarassingly") parallelizable problems scale linearly: 
***
```{r, echo=FALSE}
library(ggplot2)
library(latex2exp)
p <- ggplot(data =data.frame(x=0), mapping = aes(x=x))
fun.1 <- function(x) x*1.1
fun.2 <- function(x) x^2
fun.3 <- function(x) x*sqrt(x)

p <- p + stat_function(
          fun = fun.1,
          mapping = aes(color = "fun.1")
          ) +
    scale_color_manual(name = "Functions",
                       values = c("blue", "red", "purple"), # Color specification
                       labels = unname(TeX(c("X","$X^2$","$X*\\sqrt(X)$"))))
p +     scale_x_continuous(limits = c(0, 10)) 

```
Scalability
========================================================
However, some problems scale quadratically:
***
```{r, echo=FALSE}
p <- p +   stat_function(
          fun = fun.2,
          mapping = aes(color = "fun.2")
          ) 
p +     scale_x_continuous(limits = c(0, 10)) 
```
Scalability
========================================================
So the trick is to find solutions that scale with less than quadratic increases, such as $X\sqrt(X)$:
***

```{r, echo=FALSE}
p <- p +
    stat_function(
          fun = fun.3,
          mapping = aes(color = "fun.3")
          ) 
p +     scale_x_continuous(limits = c(0, 10)) 
```
Scalability
========================================================
The reason is obvious when you let this run for large numbers (precisely where this becomes salient):
***
```{r, echo=FALSE}
p +     scale_x_continuous(limits = c(0, 1000)) 
```


Summary
=======
- Reproducibility and High-performance computing have a lot of overlap
- we will discuss elements of both over the next three days

***
```{r, echo=FALSE}
require(venneuler)
v <- venneuler(c(Repro=500, HPC=400, "Repro&HPC"=1000))
plot(v, col=c("red","blue","purple"))
```